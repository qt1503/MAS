# MathQA_MAS - Math Question Answering_Multi Agent System

An intelligent math question answering system utilizing various AI methods with logical reasoning and step-by-step problem-solving capabilities.

## ğŸ¯ Objectives

Develop a multi-agent system to answer math questions with a focus on:
- **Logical reasoning**: Analyze and understand the essence of the problem
- **Step-by-step solutions**: Provide detailed, easy-to-understand explanations
- **High accuracy**: Ensure correct results for various types of math problems
- **Versatile support**: Serve students, teachers, and math enthusiasts

## ğŸ”¬ Supported Prompting Methods

The system implements 5 advanced prompting methods:

1. **Zero-shot** - Direct answering without sample examples
2. **CoT (Chain of Thought)** - Step-by-step logical reasoning
3. **PoT (Program of Thoughts)** - Generate Python code for computation
4. **PaL (Program-aided Language)** - Combine natural language and programming
5. **MultiAgent** - Multiple agents collaborate to solve problems

## ğŸ“Š Datasets

The project uses 3 international benchmark datasets:

- **[GSM8K](https://github.com/openai/grade-school-math)** - 8,500 high-quality math problems for elementary students
- **[TATQA](https://github.com/NExTplusplus/TAT-QA)** - 16,552 financial report questions from 2,757 real-world contexts
- **[TABMWP](https://github.com/lupantech/PromptPG)** - 38,431 problems requiring reasoning over both text and tables

## ğŸš€ Installation & Setup

### System Requirements
- Python 3.8+
- OpenAI API key (or another LLM provider's API key)

### 1. Clone the repository

```bash
git clone https://github.com/qt1503/MAS.git
cd MAS
```

### 2. Create a virtual environment (recommended)

```bash
python -m venv venv

# macOS/Linux
source venv/bin/activate

# Windows
venv\Scripts\activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables

```bash
cp env.example .env
```

Edit the `.env` file with your information:

```env
# OpenAI API Configuration  
OPENAI_API_KEY="your-openai-api-key-here"

# LangSmith Tracing (optional for advanced tracking)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY="your-langsmith-api-key-here"
LANGSMITH_PROJECT="your-project-name"

# Model Configuration
MODEL_NAME="gpt-4"
MODEL_PROVIDER="openai"
TEMPERATURE=0.2
```

## ğŸ’» Usage

### Run single experiments

```bash
# Run few-shot PoT on a specific dataset
python few_shot_PoT.py

# Run a specific method via the mathqa module
python mathqa.py
```

### Evaluate on datasets

First, create the dataset on LangSmith:

```bash
# Create a specific dataset
python mathqa.py create-dataset --dataset GSM8K --limit 100

# Create all datasets
python mathqa.py create-dataset --dataset all --limit 50
```

Then run evaluation:

```bash
# Evaluate with Zero-shot method
python mathqa.py test --method Zero_shot --dataset GSM8K

# Evaluate with CoT method
python mathqa.py test --method CoT --dataset TATQA
```

### List available datasets

```bash
python mathqa.py datasets
```

### Generate analysis charts

```bash
# Generate comparison charts for methods
python __main__.py --metric comparison --methods Zero-shot CoT PoT PaL --datasets GSM8K TATQA --save

# Generate cost charts
python __main__.py --metric cost --methods CoT PoT --datasets GSM8K --save
```

## ğŸ“ Project Structure

```
MAS/
â”œâ”€â”€ __main__.py              # Main script for visualization and analysis
â”œâ”€â”€ mathqa.py                # Utility module for mathematical operations  
â”œâ”€â”€ few_shot_PoT.py          # Few-shot Program of Thoughts implementation
â”œâ”€â”€ requirements.txt         # Required dependencies
â”œâ”€â”€ env.example              # Environment configuration template
â”œâ”€â”€ README_DATASET.md        # Detailed dataset documentation
â”œâ”€â”€ datasets/                # Raw datasets
â”‚   â”œâ”€â”€ GSM8K/               # Grade School Math dataset
â”‚   â”‚   â”œâ”€â”€ test.jsonl
â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”œâ”€â”€ test_socratic.jsonl
â”‚   â”‚   â”œâ”€â”€ train_socratic.jsonl
â”‚   â”‚   â””â”€â”€ example_model_solutions.jsonl
â”‚   â”œâ”€â”€ TATQA/               # Table and Text QA dataset  
â”‚   â”‚   â”œâ”€â”€ tatqa_dataset_train.json
â”‚   â”‚   â”œâ”€â”€ tatqa_dataset_dev.json
â”‚   â”‚   â”œâ”€â”€ tatqa_dataset_test.json
â”‚   â”‚   â””â”€â”€ tatqa_dataset_test_gold.json
â”‚   â””â”€â”€ TABMWP/              # Tabular Math Word Problems
â”‚       â”œâ”€â”€ algorithm.png
â”‚       â”œâ”€â”€ dataset.png
â”‚       â”œâ”€â”€ prediction.png
â”‚       â”œâ”€â”€ promptpg.png
â”‚       â””â”€â”€ tabmwp/
â”œâ”€â”€ mint/                    # Core framework package
â”‚   â”œâ”€â”€ __init__.py          # Package initialization
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ Zero_shot.py         # Zero-shot implementation
â”‚   â”œâ”€â”€ CoT.py               # Chain of Thought implementation  
â”‚   â”œâ”€â”€ PoT.py               # Program of Thoughts implementation
â”‚   â”œâ”€â”€ PaL.py               # Program-aided Language implementation
â”‚   â”œâ”€â”€ chart.py             # Chart generation utilities
â”‚   â”œâ”€â”€ dataset_to_langsmith.py  # LangSmith integration tools
â”‚   â””â”€â”€ testing/             # Testing modules
â”‚       â””â”€â”€ CoT.py
â”œâ”€â”€ EDA/                     # Exploratory Data Analysis notebooks
â”‚   â”œâ”€â”€ GSM8K_EDA.ipynb
â”‚   â”œâ”€â”€ TATQA_EDA.ipynb
â”‚   â””â”€â”€ TABMWP_EDA.ipynb
â”œâ”€â”€ results/                 # Processed results from experiments
â”‚   â”œâ”€â”€ CoT_GSM8K.json
â”‚   â”œâ”€â”€ CoT_TABMWP.json
â”‚   â”œâ”€â”€ CoT_TATQA.json
â”‚   â”œâ”€â”€ MultiAgent_GSM8K.json
â”‚   â”œâ”€â”€ MultiAgent_TABMWP.json
â”‚   â”œâ”€â”€ MultiAgent_TATQA.json
â”‚   â”œâ”€â”€ PaL_GSM8K.json
â”‚   â”œâ”€â”€ PaL_TATQA.json
â”‚   â”œâ”€â”€ PoT_GSM8K.json
â”‚   â”œâ”€â”€ PoT_TABMWP.json
â”‚   â”œâ”€â”€ PoT_TATQA.json
â”‚   â”œâ”€â”€ Zero-shot_GSM8K.json
â”‚   â”œâ”€â”€ Zero-shot_TABMWP.json
â”‚   â””â”€â”€ Zero-shot_TATQA.json
â””â”€â”€ save_log/                # Detailed logs with timestamps
    â”œâ”€â”€ CoT_results_gsm8k_26-07-2025_16:36:03_500samples.json
    â”œâ”€â”€ CoT_results_tabmwp_26-07-2025_18:12:37_500samples.json
    â”œâ”€â”€ CoT_results_tatqa_26-07-2025_17:01:36_500samples.json
    â”œâ”€â”€ MultiAgent_results_gsm8k_27-07-2025_01:40:00_500samples.json
    â”œâ”€â”€ MultiAgent_results_tabmwp_27-07-2025_00:54:48_500samples.json
    â”œâ”€â”€ MultiAgent_results_tatqa_27-07-2025_02:32:28_500samples.json
    â”œâ”€â”€ PaL_results_gsm8k_26-07-2025_19:01:30_500samples.json
    â”œâ”€â”€ PaL_results_tabmwp_26-07-2025_18:33:35_500samples.json
    â”œâ”€â”€ PaL_results_tatqa_26-07-2025_19:18:41_500samples.json
    â”œâ”€â”€ PoT_results_gsm8k_26-07-2025_14:34:04_500samples.json
    â”œâ”€â”€ PoT_results_tabmwp_26-07-2025_15:37:25_500samples.json
    â”œâ”€â”€ PoT_results_tatqa_26-07-2025_14:52:34_500samples.json
    â”œâ”€â”€ Zero-shot_results_gsm8k_26-07-2025_13:26:24_500samples.json
    â”œâ”€â”€ Zero-shot_results_tabmwp_26-07-2025_15:45:06_500samples.json
    â””â”€â”€ Zero-shot_results_tatqa_26-07-2025_13:36:44_500samples.json
```

## ğŸ”§ Advanced Features

### Sandbox Code Execution
The system uses a secure sandbox environment to execute Python code:
- **Isolated Environment**: Code runs in an isolated environment for system safety
- **Timeout Protection**: Execution time limits to prevent infinite loops
- **Resource Limiting**: Controls memory and CPU usage
- **Safe Libraries**: Only allows importing safe math/science libraries
- **Error Recovery**: Automatically detects and fixes syntax errors with a retry mechanism

### Advanced Visualization
The framework provides a powerful visualization system:
- **Dual-axis plots**: Display accuracy and other metrics simultaneously
- **Multi-dataset comparison**: Compare performance across multiple datasets
- **Custom color schemes**: Clearly distinguish methods with highlight colors
- **Statistical annotations**: Show exact values on charts
- **Publication-ready charts**: Export high-quality visualizations

### LangSmith Integration
Comprehensive integration with the LangSmith ecosystem:
- **Experiment Tracking**: Automatically logs all runs with metadata
- **Cost Monitoring**: Real-time tracking of API call costs
- **Performance Analytics**: Analyze accuracy and latency for each method
- **Dataset Versioning**: Manage versions of datasets and results
- **Traceability**: Full tracing from input to output for debugging

### Error Handling & Debugging
Intelligent debugging system with multiple layers:
- **Syntax Error Detection**: Automatically detects and fixes Python syntax errors
- **Logic Error Recovery**: Attempts to fix logical errors in generated code
- **Retry Mechanism**: Limits debug attempts to avoid infinite loops
- **Detailed Logging**: Detailed logs for each step to analyze issues
- **Graceful Degradation**: Fallback strategies when code execution fails

## ğŸ“Š Performance Evaluation

The system is evaluated on the following criteria:
- **Accuracy**: Correct answer rate
- **Cost**: API call cost
- **Latency**: Response time
- **Token Usage**: Number of tokens used

## ğŸ”— Contact

- GitHub: [@qt1503](https://github.com/qt1503), [@nguyenmanhtuan2004](https://github.com/nguyenmanhtuan2004)

## ğŸ™ Acknowledgments

- [OpenAI](https://openai.com/) for GPT models
- [LangChain](https://langchain.com/) for the framework
- [LangSmith](https://smith.langchain.com/) for tracing and evaluation
- The authors of GSM8K, TATQA, and TABMWP datasets

